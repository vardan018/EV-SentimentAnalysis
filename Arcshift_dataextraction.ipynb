{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install praw pandas tqdm nltk spacy gensim vaderSentiment matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOg40mCS9_MA",
        "outputId": "039ce444-c312-4c4f-d893-d25b67adeb4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.12/dist-packages (from praw) (1.9.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: vaderSentiment, update_checker, prawcore, gensim, praw\n",
            "Successfully installed gensim-4.4.0 praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0 vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Module 1: Data Collection with BAScraper (ArcticShift)\n",
        "Collects Reddit data evenly distributed across 2019-2025 for EV Sentiment Analysis.\n",
        "Uses ArcticShiftAsync for comprehensive historical coverage.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "from datetime import datetime, timedelta\n",
        "# Ensure you have the BAScraper folder in your project directory\n",
        "from BAScraper.BAScraper_async import ArcticShiftAsync\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure logging\n",
        "log_dir = \"logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"ev_sentiment_collection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION: EV SENTIMENT ANALYSIS\n",
        "# ==========================================\n",
        "\n",
        "# EXPANDED SUBREDDITS LIST\n",
        "# Includes general EV, specific brands (Tesla, Rivian, Hyundai),\n",
        "# infrastructure specific, and general car advice.\n",
        "SUBREDDITS = [\n",
        "    'electricvehicles',   # General EV discussion\n",
        "    'cars',               # General automotive (mix of pro/anti EV)\n",
        "    'TeslaMotors',        # Tesla news/official\n",
        "    'TeslaLounge',        # Casual Tesla discussion\n",
        "    'RealTesla',          # Critical/Skeptical view (Important for negative sentiment)\n",
        "    'Rivian',             # Competitor brand specific\n",
        "    'Ioniq5',             # Specific popular model (Hyundai)\n",
        "    'evcharging',         # Infrastructure specific (Great for \"charging\" keyword)\n",
        "    'whatcarshouldibuy',  # Purchase advice (Great for \"range anxiety\")\n",
        "    'technology'          # General tech perspective on EVs\n",
        "]\n",
        "\n",
        "# Keywords identified in your plan\n",
        "KEYWORDS = ['EV', 'charging', 'battery', 'range anxiety']\n",
        "\n",
        "# Time periods for even distribution (2019-2025)\n",
        "YEAR_PERIODS = [\n",
        "    ('2019-01-01', '2019-12-31'),\n",
        "    ('2020-01-01', '2020-12-31'),\n",
        "    ('2021-01-01', '2021-12-31'),\n",
        "    ('2022-01-01', '2022-12-31'),\n",
        "    ('2023-01-01', '2023-12-31'),\n",
        "    ('2024-01-01', '2024-12-31'),\n",
        "    ('2025-01-01', '2025-12-01'),\n",
        "]\n",
        "\n",
        "# Targets\n",
        "POSTS_PER_YEAR_TARGET = 300\n",
        "COMMENTS_SAMPLE_SIZE = 500\n",
        "\n",
        "\n",
        "class BAScraperCollector:\n",
        "    \"\"\"Reddit data collector using BAScraper with ArcticShift\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize BAScraper\"\"\"\n",
        "        logger.info(\"Initializing BAScraper (ArcticShift) Data Collector for EV Analysis\")\n",
        "\n",
        "        # Initialize ArcticShiftAsync\n",
        "        self.scraper = ArcticShiftAsync(\n",
        "            save_dir=os.getcwd(),\n",
        "            task_num=3,\n",
        "            log_level='INFO',\n",
        "            log_stream_level='INFO'\n",
        "        )\n",
        "\n",
        "        self.all_posts = []\n",
        "        self.all_comments = []\n",
        "\n",
        "    async def collect_submissions_for_period(self, subreddit, after, before, query_term):\n",
        "        \"\"\"Collect submissions for a specific period and query\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"  Searching r/{subreddit} for '{query_term}' ({after} to {before})\")\n",
        "\n",
        "            result = await self.scraper.fetch(\n",
        "                mode='submissions_search',\n",
        "                subreddit=subreddit,\n",
        "                query=query_term,\n",
        "                after=after,\n",
        "                before=before,\n",
        "                limit=100,  # 100 per request\n",
        "                sort='asc',\n",
        "                fields=['id', 'title', 'selftext', 'author', 'created_utc', 'subreddit',\n",
        "                       'score', 'num_comments', 'url', 'link_flair_text']\n",
        "            )\n",
        "\n",
        "            if result:\n",
        "                posts_list = list(result.values())\n",
        "                logger.info(f\"    Found {len(posts_list)} posts\")\n",
        "                return posts_list\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error collecting from r/{subreddit}: {e}\")\n",
        "            return []\n",
        "\n",
        "    async def collect_all_submissions(self):\n",
        "        \"\"\"Collect submissions across all years and subreddits\"\"\"\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(\"COLLECTING SUBMISSIONS\")\n",
        "        logger.info(\"=\"*80)\n",
        "\n",
        "        posts_by_year = {year.split('-')[0]: [] for year, _ in YEAR_PERIODS}\n",
        "\n",
        "        for after, before in YEAR_PERIODS:\n",
        "            year = after[:4]\n",
        "            logger.info(f\"\\n{'='*60}\")\n",
        "            logger.info(f\"YEAR: {year}\")\n",
        "            logger.info(f\"{'='*60}\")\n",
        "\n",
        "            year_posts = []\n",
        "\n",
        "            for subreddit in SUBREDDITS:\n",
        "                for keyword in KEYWORDS:\n",
        "                    posts = await self.collect_submissions_for_period(\n",
        "                        subreddit, after, before, keyword\n",
        "                    )\n",
        "                    year_posts.extend(posts)\n",
        "                    # Small delay to respect rate limits with increased subreddits\n",
        "                    await asyncio.sleep(1.5)\n",
        "\n",
        "            # Remove duplicates within year\n",
        "            unique_posts = {p['id']: p for p in year_posts}\n",
        "            posts_by_year[year] = list(unique_posts.values())\n",
        "\n",
        "            logger.info(f\"Year {year} total: {len(posts_by_year[year])} unique posts\")\n",
        "\n",
        "        # Flatten all posts\n",
        "        for year_posts in posts_by_year.values():\n",
        "            self.all_posts.extend(year_posts)\n",
        "\n",
        "        logger.info(f\"\\n{'='*80}\")\n",
        "        logger.info(f\"TOTAL SUBMISSIONS COLLECTED: {len(self.all_posts)}\")\n",
        "        logger.info(f\"{'='*80}\")\n",
        "\n",
        "        return self.all_posts\n",
        "\n",
        "    async def collect_comments_for_submissions(self, submission_ids, limit=30):\n",
        "        \"\"\"Collect comments for sampled submissions\"\"\"\n",
        "        logger.info(f\"\\nCollecting comments for {len(submission_ids)} submissions\")\n",
        "\n",
        "        all_comments = []\n",
        "\n",
        "        for i, sub_id in enumerate(submission_ids):\n",
        "            try:\n",
        "                if (i + 1) % 50 == 0:\n",
        "                    logger.info(f\"  Progress: {i+1}/{len(submission_ids)}\")\n",
        "\n",
        "                result = await self.scraper.fetch(\n",
        "                    mode='comments_search',\n",
        "                    link_id=sub_id,\n",
        "                    limit=limit,\n",
        "                    fields=['id', 'body', 'author', 'created_utc', 'subreddit',\n",
        "                           'score', 'link_id', 'parent_id']\n",
        "                )\n",
        "\n",
        "                if result:\n",
        "                    comments_list = list(result.values())\n",
        "                    all_comments.extend(comments_list)\n",
        "\n",
        "                await asyncio.sleep(0.5)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"Error collecting comments for {sub_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"Collected {len(all_comments)} comments\")\n",
        "        return all_comments\n",
        "\n",
        "    async def run_collection(self):\n",
        "        \"\"\"Main collection orchestration\"\"\"\n",
        "        logger.info(\"Starting BAScraper collection pipeline\")\n",
        "        logger.info(f\"Target Subreddits ({len(SUBREDDITS)}): {SUBREDDITS}\")\n",
        "        logger.info(f\"Target Keywords: {KEYWORDS}\")\n",
        "\n",
        "        # Collect submissions\n",
        "        await self.collect_all_submissions()\n",
        "\n",
        "        # Sample submissions for comment collection (stratified by year)\n",
        "        if not self.all_posts:\n",
        "            logger.warning(\"No posts collected. Skipping comment collection.\")\n",
        "            return [], []\n",
        "\n",
        "        df = pd.DataFrame(self.all_posts)\n",
        "        df['created_datetime'] = pd.to_datetime(df['created_utc'], unit='s')\n",
        "        df['year'] = df['created_datetime'].dt.year\n",
        "\n",
        "        # Sample posts for comments (proportional to year distribution, max 500 total)\n",
        "        sample_ids = []\n",
        "        for year in df['year'].unique():\n",
        "            year_posts = df[df['year'] == year]\n",
        "            n_sample = min(100, len(year_posts))  # Max 100 per year\n",
        "            sampled = year_posts.sample(n=n_sample, random_state=42)\n",
        "            sample_ids.extend(sampled['id'].tolist())\n",
        "\n",
        "        # Limit to 500 total\n",
        "        if len(sample_ids) > COMMENTS_SAMPLE_SIZE:\n",
        "            sample_ids = sample_ids[:COMMENTS_SAMPLE_SIZE]\n",
        "\n",
        "        logger.info(f\"\\nSampled {len(sample_ids)} posts for comment collection\")\n",
        "\n",
        "        # Collect comments\n",
        "        self.all_comments = await self.collect_comments_for_submissions(sample_ids)\n",
        "\n",
        "        return self.all_posts, self.all_comments\n",
        "\n",
        "    def save_data(self):\n",
        "        \"\"\"Save collected data to CSV files\"\"\"\n",
        "        os.makedirs('data/raw', exist_ok=True)\n",
        "\n",
        "        # Process and save posts\n",
        "        if not self.all_posts:\n",
        "            logger.error(\"No data to save.\")\n",
        "            return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "        posts_df = pd.DataFrame(self.all_posts)\n",
        "\n",
        "        # Add engagement metrics\n",
        "        posts_df['engagement_score'] = posts_df['score'] + (posts_df['num_comments'] * 2)\n",
        "\n",
        "        # Add temporal features\n",
        "        posts_df['created_datetime'] = pd.to_datetime(posts_df['created_utc'], unit='s')\n",
        "        posts_df['year'] = posts_df['created_datetime'].dt.year\n",
        "        posts_df['month'] = posts_df['created_datetime'].dt.month\n",
        "        posts_df['quarter'] = posts_df['created_datetime'].dt.quarter\n",
        "\n",
        "        posts_file = 'data/raw/ev_posts.csv'\n",
        "        posts_df.to_csv(posts_file, index=False)\n",
        "        logger.info(f\"\\n✓ Saved {len(posts_df)} posts to {posts_file}\")\n",
        "\n",
        "        # Process and save comments\n",
        "        comments_df = pd.DataFrame(self.all_comments)\n",
        "        if not comments_df.empty:\n",
        "            comments_df['created_datetime'] = pd.to_datetime(comments_df['created_utc'], unit='s')\n",
        "\n",
        "        comments_file = 'data/raw/ev_comments.csv'\n",
        "        comments_df.to_csv(comments_file, index=False)\n",
        "        logger.info(f\"✓ Saved {len(comments_df)} comments to {comments_file}\")\n",
        "\n",
        "        # Generate collection report\n",
        "        report = {\n",
        "            'collection_metadata': {\n",
        "                'collection_date': datetime.now().isoformat(),\n",
        "                'project': 'EV Sentiment Analysis',\n",
        "                'scraper': 'BAScraper (ArcticShift)',\n",
        "            },\n",
        "            'data_summary': {\n",
        "                'total_posts': len(self.all_posts),\n",
        "                'total_comments': len(self.all_comments),\n",
        "                'unique_authors_posts': posts_df['author'].nunique(),\n",
        "                'unique_subreddits': posts_df['subreddit'].nunique(),\n",
        "            },\n",
        "            'configuration': {\n",
        "                'subreddits': SUBREDDITS,\n",
        "                'keywords': KEYWORDS,\n",
        "                'year_periods': YEAR_PERIODS,\n",
        "            },\n",
        "            'subreddit_breakdown': posts_df['subreddit'].value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "        os.makedirs('data/metadata', exist_ok=True)\n",
        "        report_file = 'data/metadata/collection_report.json'\n",
        "        with open(report_file, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        logger.info(f\"✓ Saved collection report to {report_file}\")\n",
        "\n",
        "        # Print summary\n",
        "        logger.info(f\"\\n{'='*80}\")\n",
        "        logger.info(\"COLLECTION SUMMARY\")\n",
        "        logger.info(f\"{'='*80}\")\n",
        "\n",
        "        # --- SUBREDDIT USAGE REPORT ---\n",
        "        logger.info(f\"\\nSubreddits Used:\")\n",
        "        logger.info(f\"  {', '.join(SUBREDDITS)}\")\n",
        "\n",
        "        logger.info(f\"\\nTotal Posts per Subreddit:\")\n",
        "        logger.info(f\"{'-'*30}\")\n",
        "        logger.info(f\"{'Subreddit':<20} | {'Count':<8}\")\n",
        "        logger.info(f\"{'-'*30}\")\n",
        "\n",
        "        subreddit_counts = posts_df['subreddit'].value_counts()\n",
        "        for sub in SUBREDDITS:\n",
        "            # We use .get(sub, 0) in case a subreddit returned 0 results\n",
        "            count = subreddit_counts.get(sub, 0)\n",
        "            logger.info(f\"  r/{sub:<18} | {count}\")\n",
        "        logger.info(f\"{'-'*30}\")\n",
        "        # ------------------------------\n",
        "\n",
        "        logger.info(f\"\\nTemporal Distribution:\")\n",
        "        for year, count in sorted(posts_df['year'].value_counts().items()):\n",
        "            pct = (count / len(posts_df)) * 100\n",
        "            logger.info(f\"  {year}: {count:4d} posts ({pct:5.1f}%)\")\n",
        "\n",
        "        logger.info(f\"\\nEngagement Metrics:\")\n",
        "        logger.info(f\"  Mean Score: {posts_df['score'].mean():.1f}\")\n",
        "        logger.info(f\"  Mean Comments: {posts_df['num_comments'].mean():.1f}\")\n",
        "\n",
        "        return posts_df, comments_df\n",
        "\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main execution\"\"\"\n",
        "    try:\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(\"EV SENTIMENT ANALYSIS - DATA COLLECTION\")\n",
        "        logger.info(\"=\"*80)\n",
        "\n",
        "        collector = BAScraperCollector()\n",
        "        await collector.run_collection()\n",
        "        posts_df, comments_df = collector.save_data()\n",
        "\n",
        "        logger.info(f\"\\n{'='*80}\")\n",
        "        logger.info(\"✓ DATA COLLECTION COMPLETE!\")\n",
        "        logger.info(f\"{'='*80}\")\n",
        "        logger.info(f\"\\nCollected:\")\n",
        "        logger.info(f\"  - {len(posts_df)} posts\")\n",
        "        logger.info(f\"  - {len(comments_df)} comments\")\n",
        "        logger.info(f\"\\nNext step: Run preprocessing for Topic Modelling (LDA) and VADER.\")\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Collection failed: {e}\", exc_info=True)\n",
        "        return 1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sys.exit(asyncio.run(main()))"
      ],
      "metadata": {
        "id": "Qc5yRTPWRYHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}