{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR0YOxO7rJNb",
        "outputId": "cd39b715-fc38-410c-ada2-1331159ac6e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m\u2714 Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m\u26a0 Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Research Questions & Scope\n",
        "To understand the public sentiment and discussion trends regarding Electric Vehicles (EVs), this analysis addresses:\n",
        "\n",
        "1.  **Volume & Growth**: How has the interest in EVs changed over time (Monthly/Quarterly)?\n",
        "2.  **Community Hubs**: Which subreddits are the primary drivers (r/electricvehicles, r/cars, r/TeslaMotors)?\n",
        "3.  **Engagement**: Post vs. Comment volume comparison.\n",
        "4.  **Topic Evolution**: Yearly analysis of discussed topics.\n",
        "5.  **Spread & Sentiment**: Distribution of sentiment and data spread.\n",
        "\n",
        "**Subreddits Analyzed**:\n",
        "-   `r/electricvehicles`\n",
        "-   `r/cars`\n",
        "-   `r/TeslaMotors`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "preprocess_and_eda_plots_enhanced.py\n",
        "\"\"\"\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLP Imports\n",
        "try:\n",
        "    import spacy\n",
        "    SPACY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SPACY_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    VADER_AVAILABLE = True\n",
        "except ImportError:\n",
        "    VADER_AVAILABLE = False\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# ---------------------------\n",
        "# Path Configuration\n",
        "# ---------------------------\n",
        "# Attempt to locate data relative to this notebook/script\n",
        "# Assuming structure:\n",
        "#  root/\n",
        "#    CSS-datafiles/outputs/ (data here)\n",
        "#    CSS-preprocessed data/ (this notebook here)\n",
        "\n",
        "DATA_DIR = Path(\"../CSS-datafiles/outputs\")\n",
        "if not DATA_DIR.exists():\n",
        "    # Fallback to local or /content\n",
        "    if Path(\"outputs\").exists():\n",
        "        DATA_DIR = Path(\"outputs\")\n",
        "    else:\n",
        "        DATA_DIR = Path(\"/content\")\n",
        "\n",
        "POSTS_FILE = DATA_DIR / \"posts_final.csv\"\n",
        "COMMENTS_FILE = DATA_DIR / \"comments_final.csv\"\n",
        "\n",
        "OUTPUT_DIR = Path(\"processed_outputs\")\n",
        "PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EDA_OUTPUT = OUTPUT_DIR / \"eda_summary.csv\"\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers\n",
        "# ---------------------------\n",
        "URL_REGEX = re.compile(r\"http\\S+|www\\.\\S+\")\n",
        "HTML_ENTITY = re.compile(r\"&\\w+;\")\n",
        "SUBREDDIT_RE = re.compile(r\"r\\/[A-Za-z0-9_]+\")\n",
        "USER_RE = re.compile(r\"u\\/[A-Za-z0-9_\\-]+\")\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text): return \"\"\n",
        "    text = str(text)\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "    text = URL_REGEX.sub(\" \", text)\n",
        "    text = HTML_ENTITY.sub(\" \", text)\n",
        "    text = SUBREDDIT_RE.sub(\" \", text)\n",
        "    text = USER_RE.sub(\" \", text)\n",
        "    text = re.sub(r\"\\[([^\\]]+)\\]\\([^)]+\\)\", r\"\\1\", text)\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.lower()\n",
        "\n",
        "def prepare_nlp():\n",
        "    if SPACY_AVAILABLE:\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
        "            print(\"[INFO] Using spaCy for preprocessing.\")\n",
        "            def proc(text):\n",
        "                doc = nlp(text)\n",
        "                return [tok.lemma_.lower() for tok in doc if tok.is_alpha and not tok.is_stop and len(tok)>2]\n",
        "            return proc, \"spaCy\"\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    # Fallback NLTK\n",
        "    for pkg in [\"punkt\", \"stopwords\"]:\n",
        "        try: nltk.data.find(f\"tokenizers/{pkg}\" if pkg==\"punkt\" else f\"corpora/{pkg}\")\n",
        "        except: nltk.download(pkg, quiet=True)\n",
        "    \n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    stemmer = PorterStemmer()\n",
        "    print(\"[INFO] Using NLTK for preprocessing.\")\n",
        "    def proc(text):\n",
        "        toks = word_tokenize(text)\n",
        "        toks = [t.lower() for t in toks if t.isalpha() and len(t)>2 and t.lower() not in stop_words]\n",
        "        toks = [stemmer.stem(t) for t in toks]\n",
        "        return toks\n",
        "    return proc, \"NLTK\"\n",
        "\n",
        "def top_n_tokens(token_lists, n=20):\n",
        "    c = Counter()\n",
        "    for toks in token_lists: c.update(toks)\n",
        "    return c.most_common(n)\n",
        "\n",
        "def ensure_vader():\n",
        "    if VADER_AVAILABLE: return SentimentIntensityAnalyzer()\n",
        "    return None\n",
        "\n",
        "def read_or_empty(path):\n",
        "    if not path.exists():\n",
        "        print(f\"[WARN] {path} not found.\")\n",
        "        return pd.DataFrame()\n",
        "    return pd.read_csv(path, low_memory=False)\n",
        "\n",
        "def save_bar(top_list, title, fname, top_n=30):\n",
        "    if not top_list: return\n",
        "    tokens, counts = zip(*top_list[:top_n])\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.bar(range(len(tokens)), counts)\n",
        "    plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fname)\n",
        "    plt.close()\n",
        "\n",
        "def save_hist(data, title, fname, xlabel=\"\", bins=50):\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.hist(data.dropna(), bins=bins)\n",
        "    plt.title(title)\n",
        "    if xlabel: plt.xlabel(xlabel)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fname)\n",
        "    plt.close()\n",
        "\n",
        "# ---------------------------\n",
        "# Enhanced Plotting Functions\n",
        "# ---------------------------\n",
        "def plot_posts_vs_comments_bar(n_posts, n_comments):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.bar([\"Posts\", \"Comments\"], [n_posts, n_comments], color=[\"skyblue\", \"orange\"])\n",
        "    plt.title(\"Number of Posts vs Comments\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(PLOTS_DIR / \"posts_vs_comments_sidebyside.png\")\n",
        "    plt.close()\n",
        "    print(\"[SAVED] posts_vs_comments_sidebyside.png\")\n",
        "\n",
        "def plot_spread(df, name):\n",
        "    # Box plots for numeric cols\n",
        "    numerics = [\"score\", \"num_comments\", \"token_count\", \"compound\"]\n",
        "    for col in numerics:\n",
        "        if col in df.columns:\n",
        "            plt.figure(figsize=(6,4))\n",
        "            try:\n",
        "                df.boxplot(column=[col], showfliers=False) # Hide outliers for better scale\n",
        "                plt.title(f\"Spread of {col} in {name}\")\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(PLOTS_DIR / f\"{name}_{col}_spread_boxplot.png\")\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Failed boxplot {col}: {e}\")\n",
        "\n",
        "def plot_temporal_analysis(df, name):\n",
        "    if df.empty or 'created_utc' not in df.columns: return\n",
        "    \n",
        "    # Handle timestamp\n",
        "    try:\n",
        "        # Check if numeric\n",
        "        if pd.api.types.is_numeric_dtype(df['created_utc']):\n",
        "             dt = pd.to_datetime(df['created_utc'], unit='s', errors='coerce')\n",
        "        else:\n",
        "             dt = pd.to_datetime(df['created_utc'], errors='coerce')\n",
        "    except:\n",
        "        return\n",
        "        \n",
        "    dt = dt.dropna()\n",
        "    if len(dt) < 10: return\n",
        "    \n",
        "    # Monthly\n",
        "    m_counts = dt.dt.to_period(\"M\").value_counts().sort_index()\n",
        "    if not m_counts.empty:\n",
        "        plt.figure(figsize=(12,5))\n",
        "        m_counts.plot(kind='line', marker='o')\n",
        "        plt.title(f\"{name} Volume (Monthly)\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(PLOTS_DIR / f\"{name}_monthly_trend.png\")\n",
        "        plt.close()\n",
        "\n",
        "    # Quarterly\n",
        "    q_counts = dt.dt.to_period(\"Q\").value_counts().sort_index()\n",
        "    if not q_counts.empty:\n",
        "        plt.figure(figsize=(10,5))\n",
        "        q_counts.plot(kind='bar', color='teal')\n",
        "        plt.title(f\"{name} Volume (Quarterly)\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(PLOTS_DIR / f\"{name}_quarterly_trend.png\")\n",
        "        plt.close()\n",
        "    \n",
        "    print(f\"[SAVED] Temporal plots for {name}\")\n",
        "\n",
        "def run_yearly_lda(df, text_col, name, num_topics=5):\n",
        "    print(f\"Starting Yearly LDA for {name}...\")\n",
        "    if df.empty or text_col not in df.columns: return\n",
        "    \n",
        "    # Ensure year\n",
        "    try:\n",
        "        if pd.api.types.is_numeric_dtype(df['created_utc']):\n",
        "             dt = pd.to_datetime(df['created_utc'], unit='s', errors='coerce')\n",
        "        else:\n",
        "             dt = pd.to_datetime(df['created_utc'], errors='coerce')\n",
        "    except:\n",
        "        return\n",
        "        \n",
        "    df = df.copy()\n",
        "    df['year'] = dt.dt.year\n",
        "    df = df.dropna(subset=['year'])\n",
        "    \n",
        "    years = sorted(df['year'].unique())\n",
        "    year_results = []\n",
        "    \n",
        "    # Tokenizer for LDA\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    def fast_tok(text):\n",
        "        return [w.lower() for w in word_tokenize(str(text)) if w.isalpha() and w.lower() not in stop_words and len(w)>2]\n",
        "\n",
        "    for y in years:\n",
        "        sub = df[df['year'] == y]\n",
        "        if len(sub) < 10: continue\n",
        "        \n",
        "        docs = sub[text_col].tolist()\n",
        "        tokens = [fast_tok(d) for d in docs]\n",
        "        if not tokens: continue\n",
        "        \n",
        "        dictionary = corpora.Dictionary(tokens)\n",
        "        # Filter extremes: less restricted since yearly slices are smaller\n",
        "        dictionary.filter_extremes(no_below=2, no_above=0.7)\n",
        "        corpus = [dictionary.doc2bow(t) for t in tokens]\n",
        "        \n",
        "        if not corpus: continue\n",
        "        \n",
        "        try:\n",
        "            lda = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=5, random_state=42)\n",
        "            topics_desc = []\n",
        "            for t in range(num_topics):\n",
        "                words = \", \".join([w for w, p in lda.show_topic(t, topn=5)])\n",
        "                topics_desc.append(f\"T{t}[{words}]\")\n",
        "            \n",
        "            year_results.append({\n",
        "                \"year\": int(y),\n",
        "                \"doc_count\": len(sub),\n",
        "                \"topics\": \" | \".join(topics_desc)\n",
        "            })\n",
        "            print(f\" - {int(y)} done ({len(sub)} docs)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error LDA year {y}: {e}\")\n",
        "\n",
        "    if year_results:\n",
        "        out_f = OUTPUT_DIR / f\"{name}_yearly_topics.csv\"\n",
        "        pd.DataFrame(year_results).to_csv(out_f, index=False)\n",
        "        print(f\"[SAVED] {out_f}\")\n",
        "    else:\n",
        "        print(\"No yearly results generated.\")\n",
        "\n",
        "# ---------------------------\n",
        "# Main Routine\n",
        "# ---------------------------\n",
        "def main():\n",
        "    print(f\"Loading data from {DATA_DIR}...\")\n",
        "    posts = read_or_empty(POSTS_FILE)\n",
        "    comments = read_or_empty(COMMENTS_FILE)\n",
        "    \n",
        "    # 1. Cleaning & Tokenization\n",
        "    token_proc_fn, _ = prepare_nlp()\n",
        "    \n",
        "    def process_df(df, label):\n",
        "        if df.empty: return df\n",
        "        if \"clean_text\" not in df.columns:\n",
        "            # Create clean_text\n",
        "            if \"title\" in df.columns and \"selftext\" in df.columns:\n",
        "                df[\"clean_text\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"selftext\"].fillna(\"\")).map(clean_text)\n",
        "            elif \"body\" in df.columns:\n",
        "                df[\"clean_text\"] = df[\"body\"].fillna(\"\").map(clean_text)\n",
        "            else:\n",
        "                df[\"clean_text\"] = \"\"\n",
        "        \n",
        "        # Tokenize\n",
        "        df[\"tokens\"] = df[\"clean_text\"].map(token_proc_fn)\n",
        "        df[\"token_count\"] = df[\"tokens\"].map(len)\n",
        "        return df\n",
        "\n",
        "    posts = process_df(posts, \"posts\")\n",
        "    comments = process_df(comments, \"comments\")\n",
        "    \n",
        "    # 2. Sentiment\n",
        "    sia = ensure_vader()\n",
        "    def add_sentiment(df):\n",
        "        if df.empty or not sia: return df\n",
        "        # check if already there\n",
        "        if \"compound\" in df.columns: return df\n",
        "        print(\"Computing sentiment...\")\n",
        "        scores = df[\"clean_text\"].astype(str).map(lambda t: sia.polarity_scores(t))\n",
        "        df[\"neg\"] = scores.map(lambda s: s[\"neg\"])\n",
        "        df[\"neu\"] = scores.map(lambda s: s[\"neu\"])\n",
        "        df[\"pos\"] = scores.map(lambda s: s[\"pos\"])\n",
        "        df[\"compound\"] = scores.map(lambda s: s[\"compound\"])\n",
        "        return df\n",
        "        \n",
        "    posts = add_sentiment(posts)\n",
        "    comments = add_sentiment(comments)\n",
        "    \n",
        "    # 3. Basic Plots (Token freq, etc - reusing logic concisely)\n",
        "    if not posts.empty:\n",
        "        save_bar(top_n_tokens(posts[\"tokens\"]), \"Top Terms (Posts)\", PLOTS_DIR/\"posts_top_terms.png\")\n",
        "        save_hist(posts[\"token_count\"], \"Token Count (Posts)\", PLOTS_DIR/\"posts_len_hist.png\")\n",
        "        if \"subreddit\" in posts.columns:\n",
        "            vc = posts[\"subreddit\"].value_counts().head(20)\n",
        "            plt.figure(figsize=(10,6))\n",
        "            plt.bar(vc.index, vc.values)\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.title(\"Top Subreddits\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(PLOTS_DIR/\"subreddit_dist.png\")\n",
        "            plt.close()\n",
        "            \n",
        "    # 4. ENHANCED ANALYSIS CALLS\n",
        "    print(\"Running enhanced analysis...\")\n",
        "    \n",
        "    # Posts vs Comments\n",
        "    plot_posts_vs_comments_bar(len(posts), len(comments))\n",
        "    \n",
        "    # Temporal\n",
        "    plot_temporal_analysis(posts, \"posts\")\n",
        "    plot_temporal_analysis(comments, \"comments\")\n",
        "    \n",
        "    # Spread\n",
        "    plot_spread(posts, \"posts\")\n",
        "    plot_spread(comments, \"comments\")\n",
        "    \n",
        "    # Yearly LDA\n",
        "    run_yearly_lda(posts, \"clean_text\", \"posts\")\n",
        "    \n",
        "    # Save processed CSVs\n",
        "    if not posts.empty: posts.to_csv(OUTPUT_DIR/\"posts_enhanced.csv\", index=False)\n",
        "    if not comments.empty: comments.to_csv(OUTPUT_DIR/\"comments_enhanced.csv\", index=False)\n",
        "    \n",
        "    print(\"\\nAll Done. Results in\", OUTPUT_DIR)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmOnjOS8rdnL",
        "outputId": "7670d3de-1caf-4dd5-a10e-5311ce4cdacd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using spaCy for preprocessing.\n",
            "[INFO] Tokenizer: spaCy\n",
            "[SAVED] /content/posts_cleaned.csv (1281 rows)\n",
            "[SAVED] /content/comments_cleaned.csv (194152 rows)\n",
            "[INFO] Installing vaderSentiment...\n",
            "[SAVED] /content/posts_cleaned_with_sent.csv\n",
            "[SAVED] /content/comments_cleaned_with_sent.csv\n",
            "[SAVED] EDA summary to /content/eda_summary.csv\n",
            "[SAVED] posts_top_tokens.png\n",
            "[SAVED] comments_top_tokens.png\n",
            "[SAVED] posts_tokencount_hist.png\n",
            "[SAVED] posts_charlen_hist.png\n",
            "[SAVED] comments_tokencount_hist.png\n",
            "[SAVED] comments_charlen_hist.png\n",
            "[SAVED] posts_subreddit_dist.png\n",
            "[SAVED] posts_compound_hist.png\n",
            "[SAVED] comments_compound_hist.png\n",
            "[SAVED] posts_timeseries_monthly.png\n",
            "[SAVED] comments_timeseries_monthly.png\n",
            "[SAVED] posts_token_vs_compound.png\n",
            "[SAVED] comments_token_vs_compound.png\n",
            "\n",
            "Done. Files written to: /content\n",
            " - comments_cleaned.csv\n",
            " - comments_cleaned_with_sent.csv\n",
            " - comments_final.csv\n",
            " - comments_token_freq.csv\n",
            " - eda_summary.csv\n",
            " - posts_cleaned.csv\n",
            " - posts_cleaned_with_sent.csv\n",
            " - posts_final.csv\n",
            " - posts_token_freq.csv\n",
            "\n",
            "Plots written to: /content/plots\n",
            " - comments_charlen_hist.png\n",
            " - comments_compound_hist.png\n",
            " - comments_timeseries_monthly.png\n",
            " - comments_token_vs_compound.png\n",
            " - comments_tokencount_hist.png\n",
            " - comments_top_tokens.png\n",
            " - posts_charlen_hist.png\n",
            " - posts_compound_hist.png\n",
            " - posts_subreddit_dist.png\n",
            " - posts_timeseries_monthly.png\n",
            " - posts_token_vs_compound.png\n",
            " - posts_tokencount_hist.png\n",
            " - posts_top_tokens.png\n"
          ]
        }
      ]
    }
  ]
}